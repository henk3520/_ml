## Gradient Boosting 範例程式
此程式是一個使用 Gradient Boosting 進行分類的範例，基於 Iris 數據集：
## Gradient Boosting 原理說明
Gradient Boosting 是一種集成學習方法，通過結合多個弱學習器（通常是決策樹）來構建一個強大的預測模型。其核心思想是逐步改進預測，每次迭代都專注於修正前一輪模型的錯誤。
# 原理步驟：
1.初始化模型：
  通常從一個簡單的預測開始（例如，對回歸問題可能是目標值的平均值；對分類問題可能是類別的對數機率）。
2.迭代訓練弱學習器：
  ◦計算當前模型的殘差（預測值與實際值之間的誤差）。
  ◦訓練一個新的弱學習器（通常是決策樹）來預測這些殘差。
  ◦使用梯度下降的方式，根據殘差的方向更新模型，減少損失函數（如均方誤差或交叉熵）。
3.加權組合：
  將新訓練的弱學習器加入模型，並根據學習率（learning rate）調整其貢獻。
4.重複迭代：
  重複上述步驟，直到達到指定的迭代次數（n_estimators）或損失收斂。
5.最終預測：
  最終模型是所有弱學習器的加權組合，用於進行預測。
# 關鍵參數：
  ◦n_estimators：弱學習器的數量（迭代次數）。
  ◦learning_rate：每次迭代的步長，控制每個弱學習器的貢獻。
  ◦max_depth：每個決策樹的最大深度，控制模型複雜度。
# 優點：
  ◦能夠捕捉複雜的非線性關係。
  ◦對噪音數據有較好的魯棒性。
  ◦在許多數據集上表現優異，特別是在結構化數據問題中。
# 缺點：
  ◦訓練時間較長，因為是逐步迭代。
  ◦容易過擬合，需要仔細調整參數（如 learning_rate 和 max_depth）。
# 範例說明：
  ◦上述程式使用 GradientBoostingClassifier 對 Iris 數據集進行分類。
  ◦設置了 100 棵樹（n_estimators=100）、學習率 0.1（learning_rate=0.1）和最大深度 3（max_depth=3）。
  ◦程式會輸出模型在測試集上的準確率。
